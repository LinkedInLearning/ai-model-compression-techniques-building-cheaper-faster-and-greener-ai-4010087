{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+yODB4YertEv6310doI+L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jKvtDsnekkbo","executionInfo":{"status":"ok","timestamp":1743632632377,"user_tz":420,"elapsed":132684,"user":{"displayName":"Tejas Chopra","userId":"08409025920690368673"}},"outputId":"4646ef42-5f46-4a15-a56b-cc6c28eabba1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}],"source":["!pip install torch torchvision matplotlib numpy"]},{"cell_type":"code","source":["# Knowledge Distillation Implementation\n","# A simple and effective implementation for distilling knowledge from a teacher model to a student model\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import torchvision\n","import torchvision.transforms as transforms\n","import time\n","import copy\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)\n","\n","# Check device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Define teacher and student models\n","class TeacherModel(nn.Module):\n","    \"\"\"A larger model to act as the teacher\"\"\"\n","    def __init__(self):\n","        super(TeacherModel, self).__init__()\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","\n","        # Pooling and dropout\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.dropout = nn.Dropout(0.25)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n","        self.fc2 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        # Feature extraction\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","\n","        # Flatten\n","        x = x.view(-1, 128 * 3 * 3)\n","\n","        # Classification\n","        x = F.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","\n","        return x\n","\n","    def get_features(self, x):\n","        \"\"\"Get intermediate features for additional distillation\"\"\"\n","        features = []\n","\n","        # Extract features from each layer\n","        x = F.relu(self.conv1(x))\n","        features.append(x)\n","        x = self.pool(x)\n","\n","        x = F.relu(self.conv2(x))\n","        features.append(x)\n","        x = self.pool(x)\n","\n","        x = F.relu(self.conv3(x))\n","        features.append(x)\n","        x = self.pool(x)\n","\n","        x = x.view(-1, 128 * 3 * 3)\n","        x = F.relu(self.fc1(x))\n","        features.append(x)\n","\n","        return features\n","\n","class StudentModel(nn.Module):\n","    \"\"\"A smaller model to be trained via knowledge distillation\"\"\"\n","    def __init__(self):\n","        super(StudentModel, self).__init__()\n","        # Convolutional layers (fewer filters)\n","        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n","\n","        # Pooling\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        # Fully connected layers (smaller)\n","        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        # Feature extraction\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","\n","        # Flatten\n","        x = x.view(-1, 32 * 7 * 7)\n","\n","        # Classification\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","    def get_features(self, x):\n","        \"\"\"Get intermediate features for additional distillation\"\"\"\n","        features = []\n","\n","        # Extract features from each layer\n","        x = F.relu(self.conv1(x))\n","        features.append(x)\n","        x = self.pool(x)\n","\n","        x = F.relu(self.conv2(x))\n","        features.append(x)\n","        x = self.pool(x)\n","\n","        x = x.view(-1, 32 * 7 * 7)\n","        x = F.relu(self.fc1(x))\n","        features.append(x)\n","\n","        return features\n","\n","# Knowledge distillation loss\n","class DistillationLoss(nn.Module):\n","    def __init__(self, alpha=0.5, temperature=2.0):\n","        super(DistillationLoss, self).__init__()\n","        self.alpha = alpha  # Weight for distillation loss vs standard loss\n","        self.temperature = temperature  # Temperature for softening probability distributions\n","\n","    def forward(self, student_logits, teacher_logits, labels):\n","        # Standard cross-entropy loss\n","        hard_loss = F.cross_entropy(student_logits, labels)\n","\n","        # Distillation loss: KL-divergence between soft targets from teacher and student\n","        soft_targets = F.softmax(teacher_logits / self.temperature, dim=1)\n","        soft_prob = F.log_softmax(student_logits / self.temperature, dim=1)\n","        soft_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (self.temperature ** 2)\n","\n","        # Combine the two losses\n","        loss = (1 - self.alpha) * hard_loss + self.alpha * soft_loss\n","\n","        return loss\n","\n","# Feature distillation loss - optional enhancement\n","class FeatureDistillationLoss(nn.Module):\n","    def __init__(self, beta=0.1):\n","        super(FeatureDistillationLoss, self).__init__()\n","        self.beta = beta  # Weight for feature distillation\n","\n","    def forward(self, student_features, teacher_features):\n","        # We'll implement a simple L2 distance for feature matching\n","        # For simplicity, we only use the last feature map from each\n","        loss = 0\n","\n","        # Adapt student feature dimensions to match teacher's\n","        student_last_feature = student_features[-1]\n","        teacher_last_feature = teacher_features[-1]\n","\n","        # Compute the mean squared error loss\n","        feat_loss = F.mse_loss(student_last_feature, teacher_last_feature)\n","\n","        return self.beta * feat_loss\n","\n","# Load MNIST dataset\n","def load_data(batch_size=64):\n","    transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","    ])\n","\n","    trainset = torchvision.datasets.MNIST(root='./data', train=True,\n","                                         download=True, transform=transform)\n","    trainloader = DataLoader(trainset, batch_size=batch_size,\n","                                              shuffle=True, num_workers=2)\n","\n","    testset = torchvision.datasets.MNIST(root='./data', train=False,\n","                                        download=True, transform=transform)\n","    testloader = DataLoader(testset, batch_size=batch_size,\n","                                             shuffle=False, num_workers=2)\n","    return trainloader, testloader\n","\n","# Train teacher model (standard training)\n","def train_teacher(model, trainloader, epochs=3):\n","    model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    print(\"Training teacher model...\")\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for i, data in enumerate(trainloader, 0):\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            if i % 100 == 99:\n","                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss/100:.3f}')\n","                running_loss = 0.0\n","\n","    print('Finished training teacher model')\n","    return model\n","\n","# Train student model with knowledge distillation\n","def train_student_with_distillation(student_model, teacher_model, trainloader,\n","                                   epochs=3, alpha=0.5, temperature=2.0, beta=0.0):\n","    student_model.to(device)\n","    teacher_model.to(device)\n","    teacher_model.eval()  # Teacher model is fixed\n","\n","    distill_criterion = DistillationLoss(alpha=alpha, temperature=temperature)\n","    feature_criterion = FeatureDistillationLoss(beta=beta) if beta > 0 else None\n","    optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n","\n","    print(\"Training student model with distillation...\")\n","    student_model.train()\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for i, data in enumerate(trainloader, 0):\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            optimizer.zero_grad()\n","\n","            # Get outputs from both models\n","            with torch.no_grad():\n","                teacher_outputs = teacher_model(inputs)\n","                if beta > 0:\n","                    teacher_features = teacher_model.get_features(inputs)\n","\n","            student_outputs = student_model(inputs)\n","            if beta > 0:\n","                student_features = student_model.get_features(inputs)\n","\n","            # Compute distillation loss\n","            loss = distill_criterion(student_outputs, teacher_outputs, labels)\n","\n","            # Add feature matching loss if requested\n","            if beta > 0:\n","                feature_loss = feature_criterion(student_features, teacher_features)\n","                loss += feature_loss\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            if i % 100 == 99:\n","                print(f'Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss/100:.3f}')\n","                running_loss = 0.0\n","\n","    print('Finished training student model')\n","    return student_model\n","\n","# Fine-tune student model on task-specific data\n","def fine_tune_student(model, trainloader, epochs=2):\n","    model.to(device)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Lower learning rate for fine-tuning\n","\n","    print(\"Fine-tuning student model...\")\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        for i, data in enumerate(trainloader, 0):\n","            inputs, labels = data[0].to(device), data[1].to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            if i % 100 == 99:\n","                print(f'Fine-tuning Epoch {epoch+1}, Batch {i+1}, Loss: {running_loss/100:.3f}')\n","                running_loss = 0.0\n","\n","    print('Finished fine-tuning student model')\n","    return model\n","\n","# Evaluate model accuracy\n","def evaluate_model(model, testloader):\n","    model.to(device)\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","# Count parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Measure inference time\n","def measure_inference_time(model, testloader, num_batches=10):\n","    model.to(device)\n","    model.eval()\n","\n","    # Warm-up\n","    for i, (images, _) in enumerate(testloader):\n","        if i > 5:\n","            break\n","        images = images.to(device)\n","        with torch.no_grad():\n","            _ = model(images)\n","\n","    # Measure time\n","    start_time = time.time()\n","    batch_count = 0\n","\n","    with torch.no_grad():\n","        for i, (images, _) in enumerate(testloader):\n","            if i >= num_batches:\n","                break\n","            images = images.to(device)\n","            _ = model(images)\n","            batch_count += 1\n","\n","    end_time = time.time()\n","    avg_time = (end_time - start_time) / batch_count\n","\n","    return avg_time\n","\n","# Get model size\n","def get_model_size(model):\n","    torch.save(model.state_dict(), \"temp_model.pt\")\n","    size_mb = os.path.getsize(\"temp_model.pt\") / (1024 * 1024)\n","    os.remove(\"temp_model.pt\")\n","    return size_mb\n","\n","# Save model predictions for further analysis\n","def save_predictions(model, testloader, filename):\n","    model.to(device)\n","    model.eval()\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for data in testloader:\n","            images, labels = data[0].to(device), data[1].to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    np.savez(filename, predictions=np.array(all_preds), labels=np.array(all_labels))\n","\n","# Visualize predictions\n","def plot_confusion_matrix(model_name, predictions_file):\n","    data = np.load(predictions_file)\n","    preds = data['predictions']\n","    labels = data['labels']\n","\n","    from sklearn.metrics import confusion_matrix\n","    import seaborn as sns\n","\n","    cm = confusion_matrix(labels, preds)\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.title(f'Confusion Matrix - {model_name}')\n","    plt.ylabel('True Label')\n","    plt.xlabel('Predicted Label')\n","    plt.savefig(f'{model_name}_confusion_matrix.png')\n","    plt.close()\n","\n","# Main function\n","def main():\n","    # Load data\n","    trainloader, testloader = load_data()\n","\n","    # Create and train teacher model\n","    teacher_model = TeacherModel()\n","    teacher_params = count_parameters(teacher_model)\n","    print(f\"Teacher model has {teacher_params:,} parameters\")\n","\n","    # Check if a pretrained model exists to save time\n","    if os.path.exists('teacher_model.pt'):\n","        print(\"Loading pre-trained teacher model...\")\n","        teacher_model.load_state_dict(torch.load('teacher_model.pt'))\n","    else:\n","        teacher_model = train_teacher(teacher_model, trainloader, epochs=3)\n","        torch.save(teacher_model.state_dict(), 'teacher_model.pt')\n","\n","    # Evaluate teacher model\n","    teacher_accuracy = evaluate_model(teacher_model, testloader)\n","    teacher_inference_time = measure_inference_time(teacher_model, testloader)\n","    teacher_size = get_model_size(teacher_model)\n","\n","    print(\"\\n--- Teacher Model Metrics ---\")\n","    print(f\"Accuracy: {teacher_accuracy:.2f}%\")\n","    print(f\"Parameters: {teacher_params:,}\")\n","    print(f\"Inference Time: {teacher_inference_time*1000:.2f} ms per batch\")\n","    print(f\"Model Size: {teacher_size:.2f} MB\")\n","\n","    # Create student model\n","    student_model = StudentModel()\n","    student_params = count_parameters(student_model)\n","    print(f\"\\nStudent model has {student_params:,} parameters\")\n","    print(f\"Parameter reduction: {(1 - student_params/teacher_params)*100:.1f}%\")\n","\n","    # Train student model without distillation (for comparison)\n","    standard_student = copy.deepcopy(student_model)\n","    if os.path.exists('standard_student.pt'):\n","        print(\"Loading pre-trained standard student model...\")\n","        standard_student.load_state_dict(torch.load('standard_student.pt'))\n","    else:\n","        standard_student = train_teacher(standard_student, trainloader, epochs=3)\n","        torch.save(standard_student.state_dict(), 'standard_student.pt')\n","\n","    # Evaluate standard student\n","    standard_student_accuracy = evaluate_model(standard_student, testloader)\n","    standard_student_time = measure_inference_time(standard_student, testloader)\n","    standard_student_size = get_model_size(standard_student)\n","\n","    print(\"\\n--- Standard Student Model Metrics ---\")\n","    print(f\"Accuracy: {standard_student_accuracy:.2f}%\")\n","    print(f\"Parameters: {student_params:,}\")\n","    print(f\"Inference Time: {standard_student_time*1000:.2f} ms per batch\")\n","    print(f\"Model Size: {standard_student_size:.2f} MB\")\n","\n","    # Train student with knowledge distillation\n","    distilled_student = copy.deepcopy(student_model)\n","\n","    if os.path.exists('distilled_student.pt'):\n","        print(\"Loading pre-trained distilled student model...\")\n","        distilled_student.load_state_dict(torch.load('distilled_student.pt'))\n","    else:\n","        distilled_student = train_student_with_distillation(\n","            distilled_student, teacher_model, trainloader,\n","            epochs=3, alpha=0.5, temperature=4.0)\n","        torch.save(distilled_student.state_dict(), 'distilled_student.pt')\n","\n","    # Evaluate distilled student\n","    distilled_accuracy = evaluate_model(distilled_student, testloader)\n","    distilled_time = measure_inference_time(distilled_student, testloader)\n","    distilled_size = get_model_size(distilled_student)\n","\n","    print(\"\\n--- Distilled Student Model Metrics ---\")\n","    print(f\"Accuracy: {distilled_accuracy:.2f}%\")\n","    print(f\"Parameters: {student_params:,}\")\n","    print(f\"Inference Time: {distilled_time*1000:.2f} ms per batch\")\n","    print(f\"Model Size: {distilled_size:.2f} MB\")\n","\n","    # Fine-tune the distilled student\n","    fine_tuned_student = copy.deepcopy(distilled_student)\n","\n","    if os.path.exists('fine_tuned_student.pt'):\n","        print(\"Loading pre-trained fine-tuned student model...\")\n","        fine_tuned_student.load_state_dict(torch.load('fine_tuned_student.pt'))\n","    else:\n","        fine_tuned_student = fine_tune_student(fine_tuned_student, trainloader, epochs=2)\n","        torch.save(fine_tuned_student.state_dict(), 'fine_tuned_student.pt')\n","\n","    # Evaluate fine-tuned student\n","    fine_tuned_accuracy = evaluate_model(fine_tuned_student, testloader)\n","    fine_tuned_time = measure_inference_time(fine_tuned_student, testloader)\n","\n","    print(\"\\n--- Fine-tuned Student Model Metrics ---\")\n","    print(f\"Accuracy: {fine_tuned_accuracy:.2f}%\")\n","    print(f\"Accuracy Improvement from Distillation: {distilled_accuracy - standard_student_accuracy:.2f}%\")\n","    print(f\"Accuracy Improvement from Fine-tuning: {fine_tuned_accuracy - distilled_accuracy:.2f}%\")\n","    print(f\"Inference Time: {fine_tuned_time*1000:.2f} ms per batch\")\n","\n","    # Save predictions for analysis\n","    save_predictions(teacher_model, testloader, 'teacher_preds.npz')\n","    save_predictions(standard_student, testloader, 'standard_student_preds.npz')\n","    save_predictions(distilled_student, testloader, 'distilled_student_preds.npz')\n","    save_predictions(fine_tuned_student, testloader, 'fine_tuned_student_preds.npz')\n","\n","    # Comparison summary\n","    print(\"\\n\" + \"=\"*50)\n","    print(\"KNOWLEDGE DISTILLATION SUMMARY\")\n","    print(\"=\"*50)\n","    print(f\"{'Model':<25} {'Accuracy':<10} {'Size (MB)':<12} {'Inference (ms)':<15} {'Parameters':<12}\")\n","    print(\"-\" * 75)\n","    print(f\"{'Teacher':<25} {teacher_accuracy:<10.2f} {teacher_size:<12.2f} {teacher_inference_time*1000:<15.2f} {teacher_params:,}\")\n","    print(f\"{'Student (Standard)':<25} {standard_student_accuracy:<10.2f} {standard_student_size:<12.2f} {standard_student_time*1000:<15.2f} {student_params:,}\")\n","    print(f\"{'Student (Distilled)':<25} {distilled_accuracy:<10.2f} {distilled_size:<12.2f} {distilled_time*1000:<15.2f} {student_params:,}\")\n","    print(f\"{'Student (Fine-tuned)':<25} {fine_tuned_accuracy:<10.2f} {distilled_size:<12.2f} {fine_tuned_time*1000:<15.2f} {student_params:,}\")\n","\n","    # Visualization\n","    models = ['Teacher', 'Student\\nStandard', 'Student\\nDistilled', 'Student\\nFine-tuned']\n","    accuracies = [teacher_accuracy, standard_student_accuracy, distilled_accuracy, fine_tuned_accuracy]\n","    params = [teacher_params, student_params, student_params, student_params]\n","    inference_times = [teacher_inference_time*1000, standard_student_time*1000,\n","                       distilled_time*1000, fine_tuned_time*1000]\n","\n","    # Create bar charts\n","    plt.figure(figsize=(15, 10))\n","\n","    # Accuracy comparison\n","    plt.subplot(2, 2, 1)\n","    plt.bar(models, accuracies, color=['blue', 'orange', 'green', 'red'])\n","    plt.title('Model Accuracy (%)')\n","    plt.ylabel('Accuracy')\n","\n","    # Parameter comparison\n","    plt.subplot(2, 2, 2)\n","    plt.bar(models, params, color=['blue', 'orange', 'green', 'red'])\n","    plt.title('Model Parameters')\n","    plt.ylabel('Parameters')\n","\n","    # Inference time comparison\n","    plt.subplot(2, 2, 3)\n","    plt.bar(models, inference_times, color=['blue', 'orange', 'green', 'red'])\n","    plt.title('Inference Time (ms)')\n","    plt.ylabel('Time (ms)')\n","\n","    # Size comparison\n","    sizes = [teacher_size, standard_student_size, distilled_size, distilled_size]\n","    plt.subplot(2, 2, 4)\n","    plt.bar(models, sizes, color=['blue', 'orange', 'green', 'red'])\n","    plt.title('Model Size (MB)')\n","    plt.ylabel('Size (MB)')\n","\n","    plt.tight_layout()\n","    plt.savefig('knowledge_distillation_comparison.png')\n","    plt.close()\n","\n","    # Plot confusion matrices\n","    plot_confusion_matrix('Teacher', 'teacher_preds.npz')\n","    plot_confusion_matrix('Standard_Student', 'standard_student_preds.npz')\n","    plot_confusion_matrix('Distilled_Student', 'distilled_student_preds.npz')\n","    plot_confusion_matrix('Fine_Tuned_Student', 'fine_tuned_student_preds.npz')\n","\n","    print(\"\\nVisualization saved as 'knowledge_distillation_comparison.png'\")\n","    print(\"Confusion matrices saved for each model.\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATxlcGGqklDR","executionInfo":{"status":"ok","timestamp":1743633561803,"user_tz":420,"elapsed":910874,"user":{"displayName":"Tejas Chopra","userId":"08409025920690368673"}},"outputId":"385f7597-3c94-46e8-d4d4-6e8dc2fd5255"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9.91M/9.91M [00:00<00:00, 38.1MB/s]\n","100%|██████████| 28.9k/28.9k [00:00<00:00, 1.19MB/s]\n","100%|██████████| 1.65M/1.65M [00:00<00:00, 10.7MB/s]\n","100%|██████████| 4.54k/4.54k [00:00<00:00, 5.46MB/s]"]},{"output_type":"stream","name":"stdout","text":["Teacher model has 688,138 parameters\n","Training teacher model...\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1, Batch 100, Loss: 0.606\n","Epoch 1, Batch 200, Loss: 0.158\n","Epoch 1, Batch 300, Loss: 0.110\n","Epoch 1, Batch 400, Loss: 0.086\n","Epoch 1, Batch 500, Loss: 0.072\n","Epoch 1, Batch 600, Loss: 0.074\n","Epoch 1, Batch 700, Loss: 0.070\n","Epoch 1, Batch 800, Loss: 0.067\n","Epoch 1, Batch 900, Loss: 0.054\n","Epoch 2, Batch 100, Loss: 0.043\n","Epoch 2, Batch 200, Loss: 0.030\n","Epoch 2, Batch 300, Loss: 0.049\n","Epoch 2, Batch 400, Loss: 0.042\n","Epoch 2, Batch 500, Loss: 0.041\n","Epoch 2, Batch 600, Loss: 0.041\n","Epoch 2, Batch 700, Loss: 0.049\n","Epoch 2, Batch 800, Loss: 0.037\n","Epoch 2, Batch 900, Loss: 0.041\n","Epoch 3, Batch 100, Loss: 0.030\n","Epoch 3, Batch 200, Loss: 0.030\n","Epoch 3, Batch 300, Loss: 0.028\n","Epoch 3, Batch 400, Loss: 0.030\n","Epoch 3, Batch 500, Loss: 0.035\n","Epoch 3, Batch 600, Loss: 0.030\n","Epoch 3, Batch 700, Loss: 0.030\n","Epoch 3, Batch 800, Loss: 0.029\n","Epoch 3, Batch 900, Loss: 0.027\n","Finished training teacher model\n","\n","--- Teacher Model Metrics ---\n","Accuracy: 98.80%\n","Parameters: 688,138\n","Inference Time: 75.43 ms per batch\n","Model Size: 2.63 MB\n","\n","Student model has 206,922 parameters\n","Parameter reduction: 69.9%\n","Training teacher model...\n","Epoch 1, Batch 100, Loss: 0.654\n","Epoch 1, Batch 200, Loss: 0.193\n","Epoch 1, Batch 300, Loss: 0.141\n","Epoch 1, Batch 400, Loss: 0.132\n","Epoch 1, Batch 500, Loss: 0.115\n","Epoch 1, Batch 600, Loss: 0.089\n","Epoch 1, Batch 700, Loss: 0.074\n","Epoch 1, Batch 800, Loss: 0.070\n","Epoch 1, Batch 900, Loss: 0.065\n","Epoch 2, Batch 100, Loss: 0.051\n","Epoch 2, Batch 200, Loss: 0.060\n","Epoch 2, Batch 300, Loss: 0.051\n","Epoch 2, Batch 400, Loss: 0.054\n","Epoch 2, Batch 500, Loss: 0.054\n","Epoch 2, Batch 600, Loss: 0.051\n","Epoch 2, Batch 700, Loss: 0.048\n","Epoch 2, Batch 800, Loss: 0.049\n","Epoch 2, Batch 900, Loss: 0.051\n","Epoch 3, Batch 100, Loss: 0.029\n","Epoch 3, Batch 200, Loss: 0.034\n","Epoch 3, Batch 300, Loss: 0.034\n","Epoch 3, Batch 400, Loss: 0.039\n","Epoch 3, Batch 500, Loss: 0.040\n","Epoch 3, Batch 600, Loss: 0.037\n","Epoch 3, Batch 700, Loss: 0.037\n","Epoch 3, Batch 800, Loss: 0.034\n","Epoch 3, Batch 900, Loss: 0.043\n","Finished training teacher model\n","\n","--- Standard Student Model Metrics ---\n","Accuracy: 98.80%\n","Parameters: 206,922\n","Inference Time: 66.96 ms per batch\n","Model Size: 0.79 MB\n","Training student model with distillation...\n","Epoch 1, Batch 100, Loss: 5.075\n","Epoch 1, Batch 200, Loss: 1.374\n","Epoch 1, Batch 300, Loss: 0.815\n","Epoch 1, Batch 400, Loss: 0.597\n","Epoch 1, Batch 500, Loss: 0.435\n","Epoch 1, Batch 600, Loss: 0.404\n","Epoch 1, Batch 700, Loss: 0.347\n","Epoch 1, Batch 800, Loss: 0.320\n","Epoch 1, Batch 900, Loss: 0.276\n","Epoch 2, Batch 100, Loss: 0.252\n","Epoch 2, Batch 200, Loss: 0.229\n","Epoch 2, Batch 300, Loss: 0.224\n","Epoch 2, Batch 400, Loss: 0.207\n","Epoch 2, Batch 500, Loss: 0.184\n","Epoch 2, Batch 600, Loss: 0.186\n","Epoch 2, Batch 700, Loss: 0.176\n","Epoch 2, Batch 800, Loss: 0.168\n","Epoch 2, Batch 900, Loss: 0.164\n","Epoch 3, Batch 100, Loss: 0.154\n","Epoch 3, Batch 200, Loss: 0.142\n","Epoch 3, Batch 300, Loss: 0.138\n","Epoch 3, Batch 400, Loss: 0.136\n","Epoch 3, Batch 500, Loss: 0.137\n","Epoch 3, Batch 600, Loss: 0.132\n","Epoch 3, Batch 700, Loss: 0.131\n","Epoch 3, Batch 800, Loss: 0.124\n","Epoch 3, Batch 900, Loss: 0.127\n","Finished training student model\n","\n","--- Distilled Student Model Metrics ---\n","Accuracy: 98.80%\n","Parameters: 206,922\n","Inference Time: 41.58 ms per batch\n","Model Size: 0.79 MB\n","Fine-tuning student model...\n","Fine-tuning Epoch 1, Batch 100, Loss: 0.029\n","Fine-tuning Epoch 1, Batch 200, Loss: 0.026\n","Fine-tuning Epoch 1, Batch 300, Loss: 0.027\n","Fine-tuning Epoch 1, Batch 400, Loss: 0.025\n","Fine-tuning Epoch 1, Batch 500, Loss: 0.028\n","Fine-tuning Epoch 1, Batch 600, Loss: 0.019\n","Fine-tuning Epoch 1, Batch 700, Loss: 0.026\n","Fine-tuning Epoch 1, Batch 800, Loss: 0.024\n","Fine-tuning Epoch 1, Batch 900, Loss: 0.030\n","Fine-tuning Epoch 2, Batch 100, Loss: 0.020\n","Fine-tuning Epoch 2, Batch 200, Loss: 0.014\n","Fine-tuning Epoch 2, Batch 300, Loss: 0.013\n","Fine-tuning Epoch 2, Batch 400, Loss: 0.020\n","Fine-tuning Epoch 2, Batch 500, Loss: 0.016\n","Fine-tuning Epoch 2, Batch 600, Loss: 0.018\n","Fine-tuning Epoch 2, Batch 700, Loss: 0.012\n","Fine-tuning Epoch 2, Batch 800, Loss: 0.015\n","Fine-tuning Epoch 2, Batch 900, Loss: 0.020\n","Finished fine-tuning student model\n","\n","--- Fine-tuned Student Model Metrics ---\n","Accuracy: 99.16%\n","Accuracy Improvement from Distillation: 0.00%\n","Accuracy Improvement from Fine-tuning: 0.36%\n","Inference Time: 40.70 ms per batch\n","\n","==================================================\n","KNOWLEDGE DISTILLATION SUMMARY\n","==================================================\n","Model                     Accuracy   Size (MB)    Inference (ms)  Parameters  \n","---------------------------------------------------------------------------\n","Teacher                   98.80      2.63         75.43           688,138\n","Student (Standard)        98.80      0.79         66.96           206,922\n","Student (Distilled)       98.80      0.79         41.58           206,922\n","Student (Fine-tuned)      99.16      0.79         40.70           206,922\n","\n","Visualization saved as 'knowledge_distillation_comparison.png'\n","Confusion matrices saved for each model.\n"]}]}]}